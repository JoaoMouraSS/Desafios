{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport sys\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nif not sys.warnoptions:\n    warnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:19:13.620996Z","iopub.execute_input":"2022-03-05T23:19:13.621406Z","iopub.status.idle":"2022-03-05T23:19:14.683482Z","shell.execute_reply.started":"2022-03-05T23:19:13.621329Z","shell.execute_reply":"2022-03-05T23:19:14.682739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ETL","metadata":{}},{"cell_type":"code","source":"# Somente os dados de treino e isturados (shuffle)\ndata_train = pd.read_csv(\"../input/qualityeducation/train.csv\", nrows = 1000000).sample(frac=1, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:19:14.684818Z","iopub.execute_input":"2022-03-05T23:19:14.685057Z","iopub.status.idle":"2022-03-05T23:19:34.11505Z","shell.execute_reply.started":"2022-03-05T23:19:14.685028Z","shell.execute_reply":"2022-03-05T23:19:34.113993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:19:34.116198Z","iopub.execute_input":"2022-03-05T23:19:34.11643Z","iopub.status.idle":"2022-03-05T23:19:34.134209Z","shell.execute_reply.started":"2022-03-05T23:19:34.116401Z","shell.execute_reply":"2022-03-05T23:19:34.133396Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizando os dados \ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:19:34.136167Z","iopub.execute_input":"2022-03-05T23:19:34.136557Z","iopub.status.idle":"2022-03-05T23:19:34.184322Z","shell.execute_reply.started":"2022-03-05T23:19:34.136514Z","shell.execute_reply":"2022-03-05T23:19:34.183745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv(\"../input/qualityeducation/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:19:34.185426Z","iopub.execute_input":"2022-03-05T23:19:34.185829Z","iopub.status.idle":"2022-03-05T23:20:01.433671Z","shell.execute_reply.started":"2022-03-05T23:19:34.185799Z","shell.execute_reply":"2022-03-05T23:20:01.432801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizando os dados:\ndata_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:20:01.434887Z","iopub.execute_input":"2022-03-05T23:20:01.435117Z","iopub.status.idle":"2022-03-05T23:20:01.464311Z","shell.execute_reply.started":"2022-03-05T23:20:01.435088Z","shell.execute_reply":"2022-03-05T23:20:01.463289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_n_inscricoes = data_test['NU_INSCRICAO']\ndata_test_n_inscricoes.to_csv('n_inscricoes')\ntrain = data_train.drop('NU_INSCRICAO', axis = 1)\ntest = data_test.drop('NU_INSCRICAO', axis = 1)\n\n# Verificando o tamanho dos datas_sets:\nprint(len(train))\nprint(len(test))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:20:01.465493Z","iopub.execute_input":"2022-03-05T23:20:01.465766Z","iopub.status.idle":"2022-03-05T23:20:10.517145Z","shell.execute_reply.started":"2022-03-05T23:20:01.465736Z","shell.execute_reply":"2022-03-05T23:20:10.516089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\n# Os primeiros milhoes são os dados do dataframe train e o restante é do test...","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:20:10.51931Z","iopub.execute_input":"2022-03-05T23:20:10.519553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Como eu já tinha feito tudo anteriormente manti os mesmos códigos...\ntrain = all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_na = (train.isnull().sum() / len(train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train:\n    if train[i].dtypes == 'int64':\n        train[i].fillna(value = 0)\n    elif train[i].dtypes == 'float64':\n        train[i] = train[i].fillna(value = 0) # O problema está aqui: \"pd.DataFrame(np.array(train[i]).astype(int))\". Pois etá transformando variáveis em numeros negativos.\n                                                # E o método que estou usando para transformar isso está demorando demais.\n    else:\n        train[i].fillna(value = 'None')\n        \nall_data_na = (train.isnull().sum() / len(train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variaveis_problemas = pd.DataFrame(train.isnull().sum()).rename(columns={0: 'NaN'}).query('NaN > 0').T\nvariaveis_problemas.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in variaveis_problemas:\n    train[i] = train[i].fillna('None')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando se tem algum dado faltante:\nall_data_na = (train.isnull().sum() / len(train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_NaN = train.drop(list(train.select_dtypes(include=('int64', 'float64')).columns), axis =1).fillna(0)\nNaN = list(train_NaN.select_dtypes(include='object').columns)\n\nfor i in NaN:\n    print(i, \":\", train[i].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\nfor i in NaN:\n    train[i] = encoder.fit_transform(train_NaN[i])\n\nprint(train.dtypes)\nprint(' ')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dados do tipo float está dando um pouco de trabalho...\ndados_tipo_float = []\n\nfor i in train:\n    if train[i].dtypes == 'float64':\n        dados_tipo_float.append(i)\n    else:\n        pass\n        \ndados_tipo_float","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in dados_tipo_float:\n    train[i] = np.array(train[i]).astype(int)\n    train[i] = train[i].replace(-9223372036854775808, 0)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecionando as melhores variáveis","metadata":{}},{"cell_type":"code","source":"# Selecionando as melhores variáveis independetes\n\nparticipante = train.iloc[:,:18]\nescola = train.iloc[:,18:26]\nespeciais = train.iloc[:,26:77]\nlocal_prova = train.iloc[:,77:81]\ndados_prova = train.iloc[:,81:92]\nsocioecn = train.iloc[:,92:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separando as variáveis dependentes.\n\nnota_cn = dados_prova.NU_NOTA_CN\nnota_ch = dados_prova.NU_NOTA_CH\nnota_lc = dados_prova.NU_NOTA_LC\nnota_mt = dados_prova.NU_NOTA_MT\nnota_reda = dados_prova.NU_NOTA_REDACAO","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropando as notas do dataframe:\n\ndados_prova.drop('NU_NOTA_CN',axis = 1, inplace = True)\ndados_prova.drop('NU_NOTA_CH',axis = 1, inplace = True)\ndados_prova.drop('NU_NOTA_LC',axis = 1, inplace = True)\ndados_prova.drop('NU_NOTA_MT',axis = 1, inplace = True)\ndados_prova.drop('NU_NOTA_REDACAO',axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dados_prova","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fazer isso por ultimo. O objetivo desse código é apenas separar os dataframes para fazer a modelagem...\nprint(len(train))\n\ntest = train[ntrain:]\ntrain = train[:ntrain]\n\nprint(len(test))\nprint(len(train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecionando as melhores features","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\ndef best_features(x_train, y_train):\n    clf = ExtraTreesClassifier().fit(x_train, y_train)\n    features = pd.DataFrame()\n    features['feature']=x_train.columns\n    features['importancia'] = clf.feature_importances_\n    best_features = features[features['importancia']>np.mean(features['importancia'])].sort_values(by='importancia', ascending = False)[:16].set_index('feature')\n    return best_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelando","metadata":{}},{"cell_type":"code","source":"# Pretendo usar decision tree no começo...\n\n# Preciso fazer um código para fazer de maneira automatizada esse dados....","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}